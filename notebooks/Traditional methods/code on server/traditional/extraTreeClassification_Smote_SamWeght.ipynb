{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45a3b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f'tensorflow version {tf.__version__}')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from numpy import * \n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db39a436",
   "metadata": {},
   "source": [
    "# Loading the Data & Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bc419d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataLoadSm():\n",
    "    test = pd.read_csv('Real_Tomorrow/test_real_tom_target.csv')\n",
    "    train = pd.read_csv('Real_Tomorrow/training_real_tom_target.csv')\n",
    "    val = pd.read_csv('Real_Tomorrow/validation_real_tom_target.csv')\n",
    "    print(train.head())\n",
    "    \n",
    "    #Split data\n",
    "    x_train = train[[ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']]\n",
    "    print (\"******training features******\")\n",
    "    print (x_train)\n",
    "    y_train = train[['Real_tom_lsm']]\n",
    "    print (\"******training target******\")\n",
    "    print (y_train)\n",
    "    \n",
    "    x_test = test[[ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']]\n",
    "    print (\"******test features******\")\n",
    "    print (x_test)\n",
    "    y_test = test[['Real_tom_lsm']]\n",
    "    print (\"******test target******\")\n",
    "    print (y_test)\n",
    "    \n",
    "    x_val = val[[ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']]\n",
    "    print (\"******validation features******\")\n",
    "    print (x_val)\n",
    "    y_val = val[['Real_tom_lsm']]\n",
    "    print (\"******validation target******\")\n",
    "    print (y_val)\n",
    "    \n",
    "    print('Before Smote')\n",
    "\n",
    "    dff = y_train\n",
    "    M = y_train.to_numpy()\n",
    "    # summarize dataset\n",
    "    classes = unique(M)\n",
    "    print(classes)\n",
    "    total = len(M)\n",
    "    for c in classes:\n",
    "        n_examples = len(M[M==c])\n",
    "        percent = n_examples / total * 100\n",
    "        print('> Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))\n",
    "    \n",
    "    \n",
    "    smt = SMOTE()\n",
    "    \n",
    "    X_train_sm, Y_train_sm = smt.fit_resample(x_train, y_train)\n",
    "    \n",
    "    print('After SMOTE')\n",
    "    dff = Y_train_sm\n",
    "    M = Y_train_sm.to_numpy()\n",
    "    # summarize dataset\n",
    "    classes = unique(M)\n",
    "    print(classes)\n",
    "    total = len(M)\n",
    "    for c in classes:\n",
    "        n_examples = len(M[M==c])\n",
    "        percent = n_examples / total * 100\n",
    "        print('> Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))\n",
    "        \n",
    "    return X_train_sm, Y_train_sm, x_val, y_val, x_test, y_test\n",
    "\n",
    "def dataLoad():\n",
    "    test = pd.read_csv('Real_Tomorrow/test_real_tom_target.csv')\n",
    "    train = pd.read_csv('Real_Tomorrow/training_real_tom_target.csv')\n",
    "    val = pd.read_csv('Real_Tomorrow/validation_real_tom_target.csv')\n",
    "    print(train.head())\n",
    "    \n",
    "    #Split data\n",
    "    x_train = train[[ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']]\n",
    "    print (\"******training features******\")\n",
    "    #print (x_train)\n",
    "    y_train = train[['Real_tom_lsm']]\n",
    "    print (\"******training target******\")\n",
    "    #print (y_train)\n",
    "    \n",
    "    x_test = test[[ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']]\n",
    "    print (\"******test features******\")\n",
    "    #print (x_test)\n",
    "    y_test = test[['Real_tom_lsm']]\n",
    "    print (\"******test target******\")\n",
    "    #print (y_test)\n",
    "    \n",
    "    x_val = val[[ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']]\n",
    "    print (\"******validation features******\")\n",
    "    #print (x_val)\n",
    "    y_val = val[['Real_tom_lsm']]\n",
    "    print (\"******validation target******\")\n",
    "    #print (y_val)\n",
    "    \n",
    "        \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a97d0525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0        time  latitude  longitude        vo          r     u_200  \\\n",
      "0           0  1980-01-01       0.0       20.0  0.000007  80.761185  1.909660   \n",
      "1           1  1980-01-01       0.0       22.5  0.000004  80.703650  1.165733   \n",
      "2           2  1980-01-01       0.0       25.0  0.000007  78.231514 -1.311676   \n",
      "3           3  1980-01-01       0.0       27.5  0.000010  79.631010 -3.777573   \n",
      "4           4  1980-01-01       0.0       30.0  0.000010  71.573875 -5.734505   \n",
      "\n",
      "      u_850     v_200     v_850        ttr  sst  lsm  Real_tom_lsm  \n",
      "0 -3.323872  1.687164 -1.823624 -247.54074  0.0  0.0           0.0  \n",
      "1 -2.844494  1.060593 -1.991425 -240.00592  0.0  0.0           0.0  \n",
      "2 -2.125244  3.280617 -1.931789 -223.76889  0.0  0.0           0.0  \n",
      "3 -1.122395  5.743889 -1.243538 -235.55556  0.0  0.0           0.0  \n",
      "4 -1.362953  6.514030 -0.954163 -254.03260  0.0  0.0           0.0  \n",
      "******training features******\n",
      "               vo          r      u_200     u_850      v_200     v_850  \\\n",
      "0        0.000007  80.761185   1.909660 -3.323872   1.687164 -1.823624   \n",
      "1        0.000004  80.703650   1.165733 -2.844494   1.060593 -1.991425   \n",
      "2        0.000007  78.231514  -1.311676 -2.125244   3.280617 -1.931789   \n",
      "3        0.000010  79.631010  -3.777573 -1.122395   5.743889 -1.243538   \n",
      "4        0.000010  71.573875  -5.734505 -1.362953   6.514030 -0.954163   \n",
      "...           ...        ...        ...       ...        ...       ...   \n",
      "4268766  0.000015  26.797535  25.075424 -3.653679  -1.221291  1.515594   \n",
      "4268767 -0.000006  27.766910  24.175919 -2.866638  -6.724304  0.861771   \n",
      "4268768  0.000010  29.111805  24.655510 -2.809170 -10.138817  0.051220   \n",
      "4268769  0.000006  27.833050  25.088104 -2.730087 -11.036507  0.666927   \n",
      "4268770  0.000007  15.843884  24.510345 -3.213837 -10.213325 -0.098499   \n",
      "\n",
      "               ttr        sst  \n",
      "0       -247.54074    0.00000  \n",
      "1       -240.00592    0.00000  \n",
      "2       -223.76889    0.00000  \n",
      "3       -235.55556    0.00000  \n",
      "4       -254.03260    0.00000  \n",
      "...            ...        ...  \n",
      "4268766 -273.34204  296.89227  \n",
      "4268767 -280.37018  296.03314  \n",
      "4268768 -281.05167  295.36078  \n",
      "4268769 -280.05610  295.10638  \n",
      "4268770 -279.40427  294.44766  \n",
      "\n",
      "[4268771 rows x 8 columns]\n",
      "******training target******\n",
      "         Real_tom_lsm\n",
      "0                 0.0\n",
      "1                 0.0\n",
      "2                 0.0\n",
      "3                 0.0\n",
      "4                 0.0\n",
      "...               ...\n",
      "4268766           0.0\n",
      "4268767           0.0\n",
      "4268768           0.0\n",
      "4268769           0.0\n",
      "4268770           0.0\n",
      "\n",
      "[4268771 rows x 1 columns]\n",
      "******test features******\n",
      "              vo          r      u_200     u_850      v_200     v_850  \\\n",
      "0       0.000011  80.796135  -2.052292  0.008678   4.996910 -1.678764   \n",
      "1       0.000011  77.748420  -4.445312  0.740505   7.517281  0.792618   \n",
      "2      -0.000001  71.178825  -3.778427  1.056324   9.333221  0.688252   \n",
      "3      -0.000005  73.585754  -4.695709  1.236446   9.589882  0.555519   \n",
      "4      -0.000016  72.382780  -4.002563  0.734211   5.410950 -1.086350   \n",
      "...          ...        ...        ...       ...        ...       ...   \n",
      "539482  0.000006   2.637233  33.277840  5.379345  -0.286896  5.558327   \n",
      "539483 -0.000020  23.660923  34.272537  6.438683 -13.026535  2.857349   \n",
      "539484 -0.000019  46.051540  35.755882  7.248966 -18.870102 -3.349407   \n",
      "539485 -0.000014  55.855648  34.069664  6.349327 -18.801796 -8.172478   \n",
      "539486  0.000006  61.602300  29.167267  4.805676 -15.093590 -9.708778   \n",
      "\n",
      "              ttr        sst  \n",
      "0      -272.04962    0.00000  \n",
      "1      -250.63333    0.00000  \n",
      "2      -229.52519    0.00000  \n",
      "3      -240.80815    0.00000  \n",
      "4      -262.45557    0.00000  \n",
      "...           ...        ...  \n",
      "539482 -277.60870  294.14987  \n",
      "539483 -270.80573  294.23798  \n",
      "539484 -249.43092  294.26890  \n",
      "539485 -239.36870  294.36630  \n",
      "539486 -251.60574  293.74924  \n",
      "\n",
      "[539487 rows x 8 columns]\n",
      "******test target******\n",
      "        Real_tom_lsm\n",
      "0                0.0\n",
      "1                0.0\n",
      "2                0.0\n",
      "3                0.0\n",
      "4                0.0\n",
      "...              ...\n",
      "539482           0.0\n",
      "539483           0.0\n",
      "539484           0.0\n",
      "539485           0.0\n",
      "539486           0.0\n",
      "\n",
      "[539487 rows x 1 columns]\n",
      "******validation features******\n",
      "              vo          r      u_200     u_850     v_200     v_850  \\\n",
      "0       0.000003  73.016390  -5.760780 -4.216808  6.860649 -4.352928   \n",
      "1       0.000003  74.569660  -4.942451 -3.857407  6.459419 -3.991157   \n",
      "2       0.000004  80.080090  -3.848740 -3.175144  6.303680 -3.446140   \n",
      "3       0.000012  83.676704   0.330811 -2.526569  7.235268 -2.307594   \n",
      "4       0.000011  76.225440   3.678749 -1.027561  7.020271 -0.077572   \n",
      "...          ...        ...        ...       ...       ...       ...   \n",
      "688397  0.000014  70.662056  23.560066  1.655861  9.690376  3.621418   \n",
      "688398 -0.000006  51.213654  22.381706  0.321705  9.860390 -0.099480   \n",
      "688399  0.000009  46.678970  22.464828  0.851299  7.661758 -0.725330   \n",
      "688400  0.000002  59.362090  22.364807  0.543045  5.595253 -1.542034   \n",
      "688401  0.000014  65.915980  23.052147  0.048565  1.080147 -1.203087   \n",
      "\n",
      "              ttr        sst  \n",
      "0      -212.59741    0.00000  \n",
      "1      -198.23593    0.00000  \n",
      "2      -195.83296    0.00000  \n",
      "3      -191.47444    0.00000  \n",
      "4      -191.98111    0.00000  \n",
      "...           ...        ...  \n",
      "688397 -271.57556  296.77530  \n",
      "688398 -269.94592  296.44290  \n",
      "688399 -270.18890  295.73486  \n",
      "688400 -264.07333  295.24792  \n",
      "688401 -259.79480  295.79680  \n",
      "\n",
      "[688402 rows x 8 columns]\n",
      "******validation target******\n",
      "        Real_tom_lsm\n",
      "0                0.0\n",
      "1                0.0\n",
      "2                0.0\n",
      "3                0.0\n",
      "4                0.0\n",
      "...              ...\n",
      "688397           0.0\n",
      "688398           0.0\n",
      "688399           0.0\n",
      "688400           0.0\n",
      "688401           0.0\n",
      "\n",
      "[688402 rows x 1 columns]\n",
      "Before Smote\n",
      "[0. 1.]\n",
      "> Class=0 : 4260067/4268771 (99.8%)\n",
      "> Class=1 : 8704/4268771 (0.2%)\n",
      "After SMOTE\n",
      "[0. 1.]\n",
      "> Class=0 : 4260067/8520134 (50.0%)\n",
      "> Class=1 : 4260067/8520134 (50.0%)\n",
      "   Unnamed: 0        time  latitude  longitude        vo          r     u_200  \\\n",
      "0           0  1980-01-01       0.0       20.0  0.000007  80.761185  1.909660   \n",
      "1           1  1980-01-01       0.0       22.5  0.000004  80.703650  1.165733   \n",
      "2           2  1980-01-01       0.0       25.0  0.000007  78.231514 -1.311676   \n",
      "3           3  1980-01-01       0.0       27.5  0.000010  79.631010 -3.777573   \n",
      "4           4  1980-01-01       0.0       30.0  0.000010  71.573875 -5.734505   \n",
      "\n",
      "      u_850     v_200     v_850        ttr  sst  lsm  Real_tom_lsm  \n",
      "0 -3.323872  1.687164 -1.823624 -247.54074  0.0  0.0           0.0  \n",
      "1 -2.844494  1.060593 -1.991425 -240.00592  0.0  0.0           0.0  \n",
      "2 -2.125244  3.280617 -1.931789 -223.76889  0.0  0.0           0.0  \n",
      "3 -1.122395  5.743889 -1.243538 -235.55556  0.0  0.0           0.0  \n",
      "4 -1.362953  6.514030 -0.954163 -254.03260  0.0  0.0           0.0  \n",
      "******training features******\n",
      "******training target******\n",
      "******test features******\n",
      "******test target******\n",
      "******validation features******\n",
      "******validation target******\n"
     ]
    }
   ],
   "source": [
    "x_train_sm, y_train_sm, x_val, y_val, x_test, y_test = dataLoadSm()\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = dataLoad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996537ab",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db091c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#scaled_x_training = scaler.fit_transform(x_training)\n",
    "#df = pd.DataFrame(StandardScaler().fit_transform(x_training))\n",
    "\n",
    "X_train_stand = x_train_sm.copy() #smote\n",
    "X_valid_stand = x_val.copy() #smote\n",
    "X_test_stand = x_test.copy() #smote\n",
    "\n",
    "#not smote\n",
    "X_train_scaled = x_train.copy() \n",
    "X_valid_scaled = x_val.copy()\n",
    "X_test_scaled = x_test.copy()\n",
    "\n",
    "num_cols = [ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']\n",
    "\n",
    "# apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "    \n",
    "    # fit on training data column\n",
    "    scale = StandardScaler().fit(X_train_stand[[i]])\n",
    "    \n",
    "    # transform the training data column\n",
    "    X_train_stand[i] = scale.transform(X_train_stand[[i]])\n",
    "    \n",
    "    # transform the training data column\n",
    "    X_valid_stand[i] = scale.transform(X_valid_stand[[i]])   \n",
    "\n",
    "    # transform the testing data column\n",
    "    X_test_stand[i] = scale.transform(X_test_stand[[i]])\n",
    "    \n",
    "for i in num_cols:\n",
    "    \n",
    "    # fit on training data column\n",
    "    scale = StandardScaler().fit(X_train_scaled[[i]])\n",
    "    \n",
    "    # transform the training data column\n",
    "    X_train_scaled[i] = scale.transform(X_train_scaled[[i]])\n",
    "    \n",
    "    # transform the training data column\n",
    "    X_valid_scaled[i] = scale.transform(X_valid_scaled[[i]])   \n",
    "\n",
    "    # transform the testing data column\n",
    "    X_test_scaled[i] = scale.transform(X_test_scaled [[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff60e27",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f263c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    #n_trees = [10, 50, 100, 500, 1000, 5000]\n",
    "    n_trees = [1]\n",
    "    min_samples_leaf = [1]\n",
    "    #    min_samples_leaf = [1, 10, 50, 100, 500, 1000, 5000]\n",
    "    max_features= ['sqrt', 'log2', None] \n",
    "    for n in n_trees:\n",
    "        for l in min_samples_leaf:\n",
    "            for m in max_features:\n",
    "                models[str(n)+'trees_'+str(l)+'leafSamples_'+str(m)+'maxFeat'] = ExtraTreesClassifier(criterion='log_loss', n_estimators=n, min_samples_leaf=l, max_features=m)\n",
    "    return models\n",
    "\n",
    "#ExtraTreesClassifier_class_weight\n",
    "def get_models_class_weight(method):\n",
    "    models_class_weight = dict()\n",
    "    #n_trees = [10, 50, 100, 500, 1000, 5000]\n",
    "    n_trees = [1]\n",
    "    min_samples_leaf = [1, 10]\n",
    "    #    min_samples_leaf = [1, 10, 50, 100, 500, 1000, 5000]\n",
    "    max_features= ['sqrt', 'log2', None] \n",
    "    \n",
    "    if method == 'balanced':\n",
    "        print(\"The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\")\n",
    "        for n in n_trees:\n",
    "            for l in min_samples_leaf:\n",
    "                for m in max_features:\n",
    "                    models_class_weight[str(n)+'trees_'+str(l)+'leafSamples_'+str(m)+'maxFeat'] = ExtraTreesClassifier(criterion='log_loss', n_estimators=n, min_samples_leaf=l, max_features=m, class_weight=\"balanced\")\n",
    "    elif method == 'balanced_subsample':\n",
    "        print(\"The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.\")\n",
    "        for n in n_trees:\n",
    "            for l in min_samples_leaf:\n",
    "                for m in max_features:\n",
    "                    models_class_weight[str(n)+'trees_'+str(l)+'leafSamples_'+str(m)+'maxFeat'] = ExtraTreesClassifier(criterion='log_loss', n_estimators=n, min_samples_leaf=l, max_features=m, class_weight=\"balanced_subsample\")\n",
    "    else:\n",
    "        print('the method is not callable')\n",
    "    return models_class_weight\n",
    "\n",
    "\n",
    "# evaluate a given model using validation\n",
    "def evaluate_model(model, x_train, y_train, x_val, y_val):\n",
    "    trained_mod = model.fit(x_train, y_train)\n",
    "    score = log_loss(y_val, model.predict_proba(x_val))\n",
    "    conf = confusion_matrix(y_val, np.round(model.predict(x_val)))\n",
    "    np.set_printoptions(precision=2)\n",
    "    #print(model)  \n",
    "    return score,conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba9432",
   "metadata": {},
   "source": [
    "# models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "027d3257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
      "The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.\n"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "models_class_weight_balanced = get_models_class_weight('balanced')\n",
    "models_class_weight_balanced_subsample = get_models_class_weight('balanced_subsample')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e9dfd",
   "metadata": {},
   "source": [
    "# random_forest_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94996b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#--------------- Smote is doing ---------------#------------#\n",
      "1trees_1leafSamples_sqrtmaxFeat\n",
      "2.0256602365596317\n",
      "[[647450  39713]\n",
      " [   661    578]]\n",
      "1trees_1leafSamples_log2maxFeat\n",
      "1.9436785447149196\n",
      "[[649347  37816]\n",
      " [   924    315]]\n",
      "1trees_1leafSamples_NonemaxFeat\n",
      "2.709961518732239\n",
      "[[633665  53498]\n",
      " [   515    724]]\n",
      "the smote is done ---->\n",
      "the results for smote is saved on results_smote.pkl\n",
      "the name for smote is saved on names_smote.pkl\n",
      "the confs for smote is saved on confs_smote.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"#------------#--------------- Smote is doing ---------------#------------#\")\n",
    "\n",
    "results1, names1, confs1 = list(), list(), list()\n",
    "for name1, model in models.items():\n",
    "    res1 = evaluate_model(model, X_train_stand, y_train_sm.values.reshape(-1,), X_valid_scaled, y_val.values.reshape(-1,))\n",
    "    print(name1)\n",
    "    print(res1[0])\n",
    "    print(res1[1])\n",
    "    results1.append(res1[0])\n",
    "    names1.append(name)\n",
    "    confs1.append(res1[1])\n",
    "    \n",
    "print(\"the smote is done ---->\")\n",
    "\n",
    "import pickle\n",
    "sample_list = results1\n",
    "file_name = \"results_smote.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sample_list, open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(\"the results for smote is saved on results_smote.pkl\")\n",
    "\n",
    "sample_list = names1\n",
    "file_name = \"names_smote.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sample_list, open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(\"the name for smote is saved on names_smote.pkl\")\n",
    "\n",
    "\n",
    "sample_list = confs1\n",
    "file_name = \"confs_smote.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sample_list, open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(\"the confs for smote is saved on confs_smote.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a50811f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#--------------- class_weight_balanced is doing ---------------#------------#\n",
      "1trees_1leafSamples_sqrtmaxFeat\n",
      "0.11820616033423816\n",
      "[[685789   1374]\n",
      " [   982    257]]\n",
      "1trees_1leafSamples_log2maxFeat\n",
      "0.12257115861483185\n",
      "[[685704   1459]\n",
      " [   984    255]]\n",
      "1trees_1leafSamples_NonemaxFeat\n",
      "0.11845702230438722\n",
      "[[685782   1381]\n",
      " [   980    259]]\n",
      "1trees_10leafSamples_sqrtmaxFeat\n",
      "0.16186666944986647\n",
      "[[650445  36718]\n",
      " [   380    859]]\n",
      "1trees_10leafSamples_log2maxFeat\n",
      "0.11119061840965049\n",
      "[[668483  18680]\n",
      " [   450    789]]\n",
      "1trees_10leafSamples_NonemaxFeat\n",
      "0.08459736522736616\n",
      "[[677573   9590]\n",
      " [   534    705]]\n",
      "the class_weight_balanced is done ---->\n",
      "the results for class_weight_balanced is saved on results_class_weight_balanced.pkl\n",
      "the name for class_weight_balanced is saved on names_class_weight_balanced.pkl\n",
      "the confs for class_weight_balanced is saved on names_class_weight_balanced.pkl\n"
     ]
    }
   ],
   "source": [
    "results2, names2, confs2 = list(), list(), list()\n",
    "\n",
    "print(\"#------------#--------------- class_weight_balanced is doing ---------------#------------#\")\n",
    "\n",
    "for name2, model in models_class_weight_balanced.items():\n",
    "    res2 = evaluate_model(model, X_train_scaled, y_train.values.reshape(-1,), X_valid_scaled, y_val.values.reshape(-1,))\n",
    "    print(name2)\n",
    "    print(res2[0])\n",
    "    print(res2[1])\n",
    "    results2.append(res2[0])\n",
    "    names2.append(name2)\n",
    "    confs2.append(res2[1])\n",
    "    \n",
    "print(\"the class_weight_balanced is done ---->\")\n",
    "\n",
    "import pickle\n",
    "sample_list = results2\n",
    "file_name = \"results_class_weight_balanced.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sample_list, open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(\"the results for class_weight_balanced is saved on results_class_weight_balanced.pkl\")\n",
    "\n",
    "sample_list = names2\n",
    "file_name = \"names_class_weight_balanced.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sample_list, open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(\"the name for class_weight_balanced is saved on names_class_weight_balanced.pkl\")\n",
    "\n",
    "sample_list = confs2\n",
    "file_name = \"confs_class_weight_balanced.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sample_list, open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(\"the confs for class_weight_balanced is saved on names_class_weight_balanced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "048947cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#--------------- class_weight_balanced_subsample is doing ---------------#------------#\n",
      "1trees_1leafSamples_sqrtmaxFeat\n",
      "0.11695185048349284\n",
      "[[685805   1358]\n",
      " [   973    266]]\n",
      "1trees_1leafSamples_log2maxFeat\n",
      "0.11705219527155249\n",
      "[[685807   1356]\n",
      " [   977    262]]\n",
      "1trees_1leafSamples_NonemaxFeat\n",
      "0.1174034020297612\n",
      "[[685826   1337]\n",
      " [  1003    236]]\n",
      "1trees_10leafSamples_sqrtmaxFeat\n",
      "0.17896676603360367\n",
      "[[650466  36697]\n",
      " [   413    826]]\n",
      "1trees_10leafSamples_log2maxFeat\n",
      "0.11982538177718811\n",
      "[[666570  20593]\n",
      " [   437    802]]\n",
      "1trees_10leafSamples_NonemaxFeat\n",
      "0.08604680168665516\n",
      "[[677242   9921]\n",
      " [   524    715]]\n",
      "the class_weight_balanced_subsample is done ---->\n",
      "the results for class_weight_balanced_subsample is saved on results_class_weight_balanced_subsample.pkl\n",
      "the name for class_weight_balanced_subsample is saved on results_class_weight_balanced_subsample.pkl\n",
      "the confs for class_weight_balanced_subsample is saved on results_class_weight_balanced_subsample.pkl\n"
     ]
    }
   ],
   "source": [
    "results3, names3, confs3 = list(), list(), list()\n",
    "print(\"#------------#--------------- class_weight_balanced_subsample is doing ---------------#------------#\")\n",
    "\n",
    "for name3, model in models_class_weight_balanced_subsample.items():\n",
    "    res3 = evaluate_model(model, X_train_scaled, y_train.values.reshape(-1,), X_valid_scaled, y_val.values.reshape(-1,))\n",
    "    print(name3)\n",
    "    print(res3[0])\n",
    "    print(res3[1])\n",
    "    results3.append(res3[0])\n",
    "    names3.append(name3)\n",
    "    confs3.append(res3[1])\n",
    "    \n",
    "    \n",
    "print(\"the class_weight_balanced_subsample is done ---->\")\n",
    "\n",
    "import pickle\n",
    "sample_list = results3\n",
    "file_name = \"results_class_weight_balanced_subsample.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sample_list, open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(\"the results for class_weight_balanced_subsample is saved on results_class_weight_balanced_subsample.pkl\")\n",
    "\n",
    "sample_list = names3\n",
    "file_name = \"names_class_weight_balanced_subsample.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sample_list, open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(\"the name for class_weight_balanced_subsample is saved on results_class_weight_balanced_subsample.pkl\")\n",
    "\n",
    "sample_list = confs3\n",
    "file_name = \"names_class_weight_balanced_subsample.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sample_list, open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(\"the confs for class_weight_balanced_subsample is saved on results_class_weight_balanced_subsample.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f0fae",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30ff4726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11695185048349284,\n",
       " 0.11705219527155249,\n",
       " 0.1174034020297612,\n",
       " 0.17896676603360367,\n",
       " 0.11982538177718811,\n",
       " 0.08604680168665516]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"results_class_weight_balanced_subsample.pkl\"\n",
    "open_file = open(file_name, \"rb\")\n",
    "loaded_list_names = pickle.load(open_file)\n",
    "open_file.close()\n",
    "loaded_list_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b759b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
