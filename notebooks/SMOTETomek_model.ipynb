{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b20d21",
   "metadata": {},
   "source": [
    "\n",
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d02b79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 1.3.5\n",
      "xarray: 0.20.1\n",
      "geopandas: 1.21.5\n",
      "tensorflow: 2.0.0\n",
      "Tensorflow/Keras: 2.2.4-tf\n",
      "pandas: 1.3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 1.0.2\n",
      "plotly: 5.6.0\n",
      "/Users/3rfanian/P_main\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "\n",
    "import xarray as xr\n",
    "print('xarray: %s' % xr.__version__)\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_rows', 150)\n",
    "\n",
    "import numpy as np\n",
    "print('geopandas: %s' % np.__version__)\n",
    "\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "print('tensorflow: %s' % tf.__version__)\n",
    "\n",
    "# Tensorflow / Keras\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense # for creating regular densely-connected NN layer.\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd # for data manipulation\n",
    "print('pandas: %s' % pd.__version__) # print version\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "import sklearn # for model evaluation\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "from sklearn.model_selection import train_test_split # for splitting the data into train and test samples\n",
    "from sklearn.metrics import classification_report # for model evaluation metrics\n",
    "\n",
    "# Visualization\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__) # print version\n",
    "\n",
    "# Other utilities\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from numpy import * \n",
    "\n",
    "\n",
    "\n",
    "# Assign main directory to a variable\n",
    "main_dir=os.path.dirname(sys.path[0])\n",
    "print(main_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581de75f",
   "metadata": {},
   "source": [
    "# DATA importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b083f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******training features******\n",
      "******training target******\n",
      "******test features******\n",
      "******test target******\n",
      "******validation features******\n",
      "******validation target******\n"
     ]
    }
   ],
   "source": [
    "test_set = pd.read_csv('test_set_with_newtarget.csv')\n",
    "validation_set = pd.read_csv('validation_set_with_newtarget.csv')\n",
    "traning_set = pd.read_csv('traning_set_with_newtarget.csv')\n",
    "\n",
    "#training with yestersay lsm\n",
    "traning_df = traning_set['newlsm']\n",
    "traning_df\n",
    "ndf = traning_df.iloc[1:]\n",
    "ndf.loc[4268771,] = 0.0\n",
    "nd = ndf.reset_index()\n",
    "del nd[\"index\"]\n",
    "nd =nd.rename(columns={'newlsm': 'Tomorrow_lsm'})\n",
    "df2 = traning_set.assign(Tomorrow_lsm=nd)\n",
    "del df2[\"Unnamed: 0\"]\n",
    "\n",
    "#validation with yestersay lsm\n",
    "validation_df = validation_set['newlsm']\n",
    "ndf_val = validation_df.iloc[1:]\n",
    "ndf_val.loc[688402,] = 0.0\n",
    "nd_val = ndf_val.reset_index()\n",
    "del nd_val[\"index\"]\n",
    "nd_val =nd_val.rename(columns={'newlsm': 'Tomorrow_lsm'})\n",
    "df3 = validation_set.assign(Tomorrow_lsm=nd_val)\n",
    "del df3[\"Unnamed: 0\"]\n",
    "\n",
    "\n",
    "#test with yestersay lsm\n",
    "test_df = test_set['newlsm']\n",
    "ndf_test = test_df.iloc[1:]\n",
    "ndf_test.loc[539487,] = 0.0\n",
    "nd_test = ndf_test.reset_index()\n",
    "del nd_test[\"index\"]\n",
    "nd_test =nd_test.rename(columns={'newlsm': 'Tomorrow_lsm'})\n",
    "df4 = test_set.assign(Tomorrow_lsm=nd_test)\n",
    "del df4[\"Unnamed: 0\"]\n",
    "\n",
    "\n",
    "training_set_wt = df2\n",
    "validation_set_wt =df3\n",
    "test_set_wt = df4\n",
    "\n",
    "\n",
    "##### Step 2 - Split training data\n",
    "x_training = training_set_wt[[ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']]\n",
    "print (\"******training features******\")\n",
    "#print (x_training)\n",
    "y_training = training_set_wt[['Tomorrow_lsm']]\n",
    "print (\"******training target******\")\n",
    "#print (y_training)\n",
    "\n",
    "##### Step 3 - Split test data\n",
    "x_test = test_set_wt[[ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']]\n",
    "print (\"******test features******\")\n",
    "#print (x_test)\n",
    "y_test = test_set_wt[['Tomorrow_lsm']]\n",
    "print (\"******test target******\")\n",
    "#print (y_test)\n",
    "\n",
    "##### Step 4 - Split validation data\n",
    "x_validation = validation_set_wt[[ 'vo', 'r', 'u_200', 'u_850', 'v_200','v_850', 'ttr','sst']]\n",
    "print (\"******validation features******\")\n",
    "#print (x_validation)\n",
    "y_validation = validation_set_wt[['Tomorrow_lsm']]\n",
    "print (\"******validation target******\")\n",
    "#print (y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2ba06",
   "metadata": {},
   "source": [
    "# SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f72d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTETomek\n",
      "[0. 1.]\n",
      "> Class=0 : 4254174/4268771 (99.7%)\n",
      "> Class=1 : 14597/4268771 (0.3%)\n",
      "After SMOTETomek\n",
      "[0. 1.]\n",
      "> Class=0 : 4254174/8508348 (50.0%)\n",
      "> Class=1 : 4254174/8508348 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Before SMOTETomek')\n",
    "\n",
    "dff = y_training\n",
    "M = y_training.to_numpy()\n",
    "# summarize dataset\n",
    "classes = unique(M)\n",
    "print(classes)\n",
    "total = len(M)\n",
    "for c in classes:\n",
    "    n_examples = len(M[M==c])\n",
    "    percent = n_examples / total * 100\n",
    "    print('> Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))\n",
    "\n",
    "smtmek = SMOTETomek(random_state=1)\n",
    "\n",
    "X_training_smtmek, Y_traning_smtmek = smtmek.fit_resample(x_training, y_training)\n",
    "\n",
    "print('After SMOTETomek')\n",
    "dff = Y_traning_smtmek\n",
    "M = Y_traning_smtmek.to_numpy()\n",
    "# summarize dataset\n",
    "classes = unique(M)\n",
    "print(classes)\n",
    "total = len(M)\n",
    "for c in classes:\n",
    "    n_examples = len(M[M==c])\n",
    "    percent = n_examples / total * 100\n",
    "    print('> Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3899eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "##### Step 6 - Use model to make predictions\n",
    "# Predict class labels on training data\n",
    "pred_labels_tr_simple_SMOTETomek_512n = (model_simple_SMOTETomek.predict(x_training))\n",
    "# Predict class labels on a test data\n",
    "pred_labels_te_simple_SMOTETomek_512n = (model_simple_SMOTETomek.predict(x_test))\n",
    "#> 0.01).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce476a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 11:42:03.665737: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-26 11:42:03.668111: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8508348 samples, validate on 688402 samples\n",
      "Epoch 1/3\n",
      "1492576/8508348 [====>.........................] - ETA: 2:13:50 - loss: 0.4360 - accuracy: 0.7873"
     ]
    }
   ],
   "source": [
    "##### Step 5 - Specify the structure of a Neural Network\n",
    "model_complex_SMOTETomek_4l_512n = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(8,)),\n",
    "    tf.keras.layers.Dense(512, activation='tanh', name='Hidden-Layer1'),\n",
    "    tf.keras.layers.Dense(512, activation='tanh', name='Hidden-Layer2'),\n",
    "    tf.keras.layers.Dense(512, activation='tanh', name='Hidden-Layer3'),\n",
    "    tf.keras.layers.Dense(512, activation='tanh', name='Hidden-Layer4'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', name='Output-Layer')\n",
    "])\n",
    "model_complex_SMOTETomek_4l_512n.compile(optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_complex_SMOTETomek_4l_512n.fit(X_training_smtmek, Y_traning_smtmek, epochs = 3 ,  validation_data=(x_validation, y_validation))#*********more epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 6 - Use model to make predictions\n",
    "# Predict class labels on training data\n",
    "pred_labels_tr_complex_SMOTETomek_512n = (model_simple_SMOTETomek.predict(x_training))\n",
    "# Predict class labels on a test data\n",
    "pred_labels_te_complex_SMOTETomek_512n = (model_simple_SMOTETomek.predict(x_test))\n",
    "#> 0.01).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9d5c32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_complex_SMOTETomek_4l_512n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6l/2k5dphhj4z30f5zpqx44561c0000gn/T/ipykernel_1517/3915955788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_complex_SMOTETomek_4l_512n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ComModel_512n_4l_SMOTETomek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_complex_SMOTETomek_4l_512n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ComModel_512n_4l_SMOTETomek.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_complex_SMOTETomek_4l_512n' is not defined"
     ]
    }
   ],
   "source": [
    "model_complex_SMOTETomek_4l_512n.save('ComModel_512n_4l_SMOTETomek')\n",
    "model_complex_SMOTETomek_4l_512n.save('ComModel_512n_4l_SMOTETomek.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6282ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq = test_set_wt.assign(predict_value=pred_labels_te_complex_SMOTETomek_512n)\n",
    "dfq.to_csv('Predicted_value_Complex_SMOTETomek_4l_512n.csv')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
